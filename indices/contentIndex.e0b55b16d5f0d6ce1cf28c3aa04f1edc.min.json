{"/":{"title":"AP Statistics Review Material","content":"\nWelcome! These are a collection of review notes for the AP Statistics course, which you're probably taking right now. I hope this helps!\n\nThese notes are currently structured by units defined by the AP Statistics curriculum that Collegeboard provides, which are split up into:\n1. [[Unit 1 - One-Variable Data Analysis]]\n2. [[Unit 2 - Two-Variable Data Analysis]] (partially completed)\n3. [[Unit 3 - Collecting Data]] (not done yet)\n4. [[Unit 4 - Probability, Random Variables, and Probability Distributions]] (not done yet)\n5. [[Unit 5 - Sampling Distributions]] (not done yet)\n6. [[Unit 6 - Inference for Categorical Data for Proportions]] (not done yet)\n7. [[Unit 7 - Inference for Quantitative Data for Means]] (not done yet)\n8. [[Unit 8 - Inference for Categorical Data for Chi-Square]]\n9. [[Unit 9 - Inference for Quantitative Data for Slopes]] (not done yet)\n10. Extra Content? (coming soon)\n\nA few bits of information about the website before you go:\n- This website is designed to be interactive, so go ahead! Click around on links to review concepts that you haven't learned yet, and have fun reviewing!\n- You can search the full text of this website by pressing `Ctrl` + `k`.\n- This website is currently *far* from complete, and there are quite a few missing ends. If you have any issues, feel free to contact me.\n- Most importantly, put your studying schedule first. If you're feeling tired, or you're cramming before a test (or the big test), take some time to de-stress every now and then.\n\n\n","lastmodified":"2022-05-13T04:31:04.776679807Z","tags":null},"/Chi-Squared-Test-for-Goodness-Of-Fit":{"title":"Chi-Squared Test for Goodness-Of-Fit","content":"The $\\chi^2$ test for goodness-of-fit is a tes tthat compares the *counts* of an observed distribution of a single *categorical variable* to the expected distribution.\n\n![[Unit 8 - Inference for Categorical Data for Chi-Square#Performing The Test]]\n\n## Hypotheses\nYou **MUST** define the null and alternate hypothesis.\n- $H_0$ {Null Hypothesis}: The given ratios are correct (context).\n- $H_A$ {Alternate Hypothesis}: The given rations are not correct (context).\n\n\u003e ==___ALWAYS PROVIDE CONTEXT.___==\n## Assumptions and Conditions\n- **Counted Data Condition**: Check that the data are counts for the *categories* of a categorical variable.\n\t- No way that this isn't satisfied; simply state the condition.\n- **Randomization Condition**: The data should come from a ==random sample== from some population.\n\t- The problem will either state that the sample was randomly selected or part of a [[Simple Random Sample|SRS]].\n\t- If not, say, \"There is no reason to believe that the sample is *not* representative of the population of (context)\".\n- **Sample Size Assumption**: We should expect to see ==at least five== individuals in each cell. Refer to the table you will provide.\n\t- Much smaller sample size compared to $z$-tests and $p$-tests.\n\nAfter proving these conditions, you **MUST** conclude with the following:\n\u003e Since the conditions are met, I will now conduct a $\\chi^2$ goodness-of-fit test.\n\n## Calculation\nFind $\\chi^2$ on your calculator, and plug in the data. Show the following:\n- $\\chi^2$ statistic\n- degrees of freedom (usually $=n-1$)\n- $p$-value\n- $\\alpha$-value (if not provided, assume $\\alpha=0.05$)\n\n### Formula\n$$\\chi^2=\\sum{\\frac{(O_i-E_i)^2}{E_i}}$$\n\u003e - $O_i$: observed value\n\u003e - $E_i$: expected value\n\n## Conclusion\nThe conclusion depends on your calculated $p$-value and the $\\alpha$-level.\n- If $p\u003c\\alpha$, **reject** the $H_0$. Therefore, there is *sufficient* evidence that $H_A$ (context).\n- If $p\u003e\\alpha$, **fail to reject** the $H_0$. Therefore, there is *insufficient* evidence that $H_A$ (context).\n\u003e Do **NOT** provide context for the *null hypothesis*.\n\u003e However, you **MUST** provide context for the *alternate hypothesis*.","lastmodified":"2022-05-13T04:31:04.772679811Z","tags":null},"/Chi-Squared-Test-for-Homogeneity":{"title":"Chi-Squared Test for Homogeneity","content":"\nA test for homogeneity tests whether one categorical variable is the same over two populations. (yet another edit)\n\n$x=0$","lastmodified":"2022-05-13T04:31:04.772679811Z","tags":null},"/Chi-Squared-Test-for-Independence":{"title":"Chi-Squared Test for Independence","content":"A test of independence examines the association between two variables in a single population.\n\n## Hypotheses\n- $H_0$: Variables are independent of each other\n\t- There is no association between row variable vs. column variable.\n- $H_A$: Variables are dependent on each other\n\t- There is an association between row variable vs. column variable.\n\n## ","lastmodified":"2022-05-13T04:31:04.772679811Z","tags":null},"/Five-Number-Summary":{"title":"Five-Number Summary","content":"The five-number summary of a dataset consists of the following:\n- minimum value\n- first/lower quartile ($Q_1$)\n- median\n- third/upper quartile ($Q_3$)\n- maximum value\n\nYou can get these values from `One-Variable Statistics` on your calculator.\n\n## Quartiles\nThe medians of the upper and lower halves of the distribution, not including the media itself in either half, and called *quartiles*.\n\nThe median of the lower half is called the *lower quartile*, or the *first quartile*. The median of the upper half is called the *upper quartile*, or the *third quartile*.\n\nThe *interquartile range* (IQR) is the difference between the third quartile and the first quartile. ^0d6775","lastmodified":"2022-05-13T04:31:04.772679811Z","tags":null},"/Linear-Regression-T-Test":{"title":"Linear Regression T-Test","content":"*Linear regression $t$ tests* are used to quantify the relationship between a\npredictor variable and a response variable.\n\nWhenever we perform a linear regression, we want to know if there is a\nstatistically significant relationship between the predictor variable\nand the response variable.\n\nWe test for significance by performing a $t$-test for the regression\nslope. We use the following null and alternative hypothesis for this\n$t$-test:\n- $H_0$: $\\beta_1=0$ (the slope is equal to zero)\n- $H_A$: $\\beta_1\\ne0$ (the slope is not equal to zero)\n\nWe then calculate the test statistic as follows:\n$$t=\\frac{b}{SE_b}$$\nwhere:\n\n-   $b$: coefficient estimate\n-   $SE_b$: standard error of the coefficient estimate\n\nIf the $p$-value that corresponds to $t$ is less than some threshold (e.g.\n$\\alpha=0.05$) then we reject the null hypothesis and conclude that there is a statistically significant relationship between the predictor variable\nand the response variable.","lastmodified":"2022-05-13T04:31:04.772679811Z","tags":null},"/Measures-of-Center-for-Quantitative-Distributions":{"title":"Measures of Center for Quantitative Distributions","content":"There are two primary measures of center, *mean* and *median*. *Mode*, which is the most commonly occurring value in the distribution, may be seen. Don't use mode.\n\n## Mean\nThe *mean* (informally the *average*) of the distribution is defined as the sum of all of the values in the distribution divided by the number of values in the distribution.\n$$\\bar{x}=\\frac{\\sum{x}}{n}$$\n$\\bar{x}$ is used for the mean of a sample (statistic), and $\\mu$ is used for mean of the population (parameter).\n\n## Median\nThe *median* of a distribution is the value in the middle of the ordered distribution. At least half of the values in the distribution are smaller than the median, and at least half of the values in the distribution are greater than the median.\n\nIf the distribution has an even number of values, the median is the mean of the two center values.\n\n## Resistance\nWhen it comes to choosing whether to use the mean or the median in a certain distribution, it is important to look at the distribution's shape, and whether it has any unusual features.\n\nIf the distribution is approximately symmetric and has no unusual features, it is fine to use either the mean or the media. However, if the distribution is skewed or has outliers, it is better to use the median, since the mean is heavily affected by extremely large or extremely small values.\n\nTherefore, the mean is defined as a *resistant statistic*, since it is not heavily influenced by extreme values. ","lastmodified":"2022-05-13T04:31:04.772679811Z","tags":null},"/Measures-of-Spread-for-Quantitative-Distributions":{"title":"Measures of Spread for Quantitative Distributions","content":"There are multiple values that can represent the spread of the distribution.\n\n## Variance and [[Standard Deviation]]\n![[Standard Deviation#^5e9145]]\nThe *variance* is the square of the [[standard deviation]].\n\n## Range\nThe *range* of the distribution is simply the difference between the greatest value and the smallest value in the distribution.\n\n\u003e Warning: If the distribution has extreme values, is extremely skewed, or has outliers, the range will not be a good representation of the spread of the data.\n\n## Interquartile Range (IQR)\n![[Five-Number Summary#^0d6775]]","lastmodified":"2022-05-13T04:31:04.772679811Z","tags":null},"/Normal-Distribution":{"title":"Normal Distribution","content":"\u003e For important calculator functions, see [[Normal Distribution Calculator Functions]].\n\n![[Pasted image 20220417195234.png]]\n\nThe *normal distribution* is a type of [[Quantitative Data#^a7c31b|continuous]] probability distribution for a random variable. The normal distribution is usually used as an idealization of many distributions of random variables.\n\nThe normal distribution is symmetrical and unimodal, centered at the mean of the sample, as seen above.\n\n\u003e Note: When doing a problem relating to the normal distribution, **always** draw a normal curve, and shade or a draw a line that relates to the solution to the problem. Make sure to notate the curve with the notation $N(\\mu,\\sigma)$.\n\n## 68-95-99.7 Rule\n\nThe *68-95-99.7 rule* (or the *empirical rule*), states the following:\n- 68% of the values in the normal distribution are within one [[Standard Deviation|standard deviation]] of the mean.\n- 95% of the values in the normal distribution are within two [[Standard Deviation|standard deviations]] of the mean.\n- 99.7% of the values in the normal distribution are within three [[Standard Deviation|standard deviations]] of the mean.\n\n## Standard Normal Distribution\n\nThe *standard normal distribution* is a special example of the normal distribution, where the mean ($\\mu$) is 0, and the standard deviation ($\\sigma$) is 1.\n\n## Mathematical Representation of the Normal Curve\n$${f(x)={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}}}$$\nDon't need this, but it's cool.","lastmodified":"2022-05-13T04:31:04.772679811Z","tags":null},"/Normal-Distribution-Calculator-Functions":{"title":"Normal Distribution Calculator Functions","content":"## Normal Cumulative Distribution Function\nThe normal cumulative distribution function (normCdf) can find the proportion of the area under the normal curve that lies between two values in the normal distribution.\n\nSimply plug in the lower and upper bounds, the mean, and the standard deviation of the function to get the proportion of the area (and therefore the proportion of the distribution) that lies between the bounds.\n\n## Inverse Normal Function\nThe inverse normal function (invNorm) is the opposite of the normal cumulative distribution function.\n\nPlug in the area, the mean, and the standard deviation to get the value that bounds the area specified.\n![[Pasted image 20220417200944.png]]","lastmodified":"2022-05-13T04:31:04.772679811Z","tags":null},"/Qualitative-Data":{"title":"Qualitative Data","content":"*Qualitative data*, or *categorical data*, are data that can be classified into a group based on a non-numerical characteristic.\n\nNote that some data can be *numerical*, but qualitative. A good example is a phone number; although it is a number, it's not a quantitative variable.","lastmodified":"2022-05-13T04:31:04.772679811Z","tags":null},"/Quantitative-Data":{"title":"Quantitative Data","content":"*Quantitative data*, or *numerical data* are measures or counts.\n\n## Discrete and Continuous Variables\n*Discrete variables* are variables that are distinct, separated values. An example is the number of people in a household; you can't have 2.5 people in a household. Therefore, this is an example of a discrete variable.\n\n*Continuous variables* are variables that can be any value in an interval. An example is the width of a sheet of paper; this could have any measurement. ^a7c31b\n\n## Describing a Quantitative Distribution\nA distribution shows the possible values for a variable and how often they occur.\n\n**The features that we mention when describing a quantitative distribution are [[Shapes of Quantitative Distributions|shape]], [[Measures of Center for Quantitative Distributions|center]], [[Measures of Spread for Quantitative Distributions|spread]], and [[Unusual Features of a Quantitative Distribution|unusual features]].** ^5bef99\n\nMake sure to review how to describe each of these features by looking at the links above; you may use different statistics to describe these features based on the distribution. ","lastmodified":"2022-05-13T04:31:04.776679807Z","tags":null},"/Shapes-of-Quantitative-Distributions":{"title":"Shapes of Quantitative Distributions","content":"Describing the shape of a quantitative distribution consists of two parts:\n- Skew\n- Modality\n\n## Skew\nIf the graph is symmetric when using the middle of the graph as the line of reflection, the graph is *approximately normal.*\n\nOtherwise, if one tail of the graph stretches out farther than the other, the histogram is said to be *skewed* to the side of the longer tail.\n\n\u003e Tip: Draw a curve along the skewed graph. The direction that you are sketching to the tail of the graph is the direction that the graph is skewed.\n\n### Formal Definition\n- Skewed left: The spread of data below the median is greater than the spread of data above the median.\n- Skewed right: The spread of data above the median is greater than the spread of data below the median.\n\n## Modality\nPeaks in a histogram are called modes; therefore, the modality of the graph describes how many \"peaks\" there are in the distribution.\n- Unimodal: one peak\n- Bimodal: two peaks\n- Multimodal: \u003e three peaks\n- Uniform: no peaks in the distribution","lastmodified":"2022-05-13T04:31:04.776679807Z","tags":null},"/Standard-Deviation":{"title":"Standard Deviation","content":"The *standard deviation* is defined as the typical difference between a value in the distribution and the average for the distribution. ^5e9145\n\nMathematically, the standard deviation is defined as the square root of the *variance* of the distribution, which is defined as the average squared deviation from the mean of the distribution. A deviation is the distance that a data value is from the mean. ^da115c\n\n$$s=\\sqrt{\\frac{1}{n-1}\\sum{(x-\\bar{x})^2}}$$\n\n\u003e If you are referring to a sample statistic, use $s$. If you are referring to a population parameter, use $\\sigma$.\n\nThe standard deviation is very useful for three reasons:\n- The standard deviation is independent of the mean.\n- The standard deviation measures the [[Measures of Spread for Quantitative Distributions|spread]] of the distribution.\n- The standard deviation is independent of the size of the distribution.\n\n### Formula for Variance\n$$s^2=\\frac{1}{n-1}\\sum{(x-\\bar{x})^2}$$","lastmodified":"2022-05-13T04:31:04.776679807Z","tags":null},"/Unit-1-One-Variable-Data-Analysis":{"title":"Unit 1 - One-Variable Data Analysis","content":"## Parameters \u0026 Statistics\nValues that describe a sample are called *statistics*, and values that describe a population are called *parameters*.\n\n## [[Quantitative Data]] \u0026 [[Qualitative Data]]\n- *Quantitative data*, or *numerical data* are measures or counts.\n- *Qualitative data*, or *categorical data,* are data that can be classified into a group based on a non-numerical characteristic.\n\nA further distinction can be made between discrete and continuous variables; see [[Quantitative Data]] for more information.\n\n## Graphical Representations for [[Qualitative Data]]\n- Bar Chart: displays the distribution of a *categorical variable*, showing the counts for each category in bars.\n\t- Relative Frequency Bar Chart: This type of bar chart displays the *relative proportion* of counts for each category.\n\t- Segmented Bar Chart: Each bar is divided *proportionally* into segments corresponding to the *percentages* of each category.\n\t- Note: Make sure to look at the axes, which will tell you whether the information is relative or absolute.\n\t- Side by Side Bar Charts: The total height for all the bars in each category should add up to 100%.\n- Pie Charts: These graphs show the whole group of categories as slices of a circle; each piece is proportional to the fraction of the whole.\n- Frequency Tables: These record the totals and the names of each category.\n\t- Relative Frequency Table: Same thing, but with percentages.\n- Two-way tables: Used to visualize two different categorical variables, and how they are distributed in relation to each other.\n\n## Graphical Representations for [[Quantitative Data]]\n- Stem \u0026 Leaf Plot: Stem-and-leaf displays show the distribution of a quantitative variable while preserving the individual values.\n- Dotplot: A dotplot places a dot along an axis for each case in the data.\n- Box-and-Whisker Plot: A box-and-whisker plot is a visualization of the [[Five-Number Summary]].\n- Histogram: A histogram plots the bin counts of a quantitative variable as the heights of bars.\n- Timeplot: A timeplot of a variable plots each observation against the time at which it was measured.\n\t- Time should always be placed on the horizontal axis.\n\n![[Quantitative Data#Describing a Quantitative Distribution]]\n\n## Ogives (Cumulative Frequency Graph Definition)\nAn *ogive* is a graph where the $p^{th}$ percentile of a distribution is the value such that $p$ percent of the observations fall at or below it.\n\n## Percentile Rank of a Term\nThe *percentile rank* of a term is the proportion of terms in the distribution that are less than or equal to that term.\n\n## [[Z-Scores]]\n![[Z-Scores#^1db326]]","lastmodified":"2022-05-13T04:31:04.776679807Z","tags":null},"/Unit-2-Two-Variable-Data-Analysis":{"title":"Unit 2 - Two-Variable Data Analysis","content":"Two-variable (or *bivariate*) [[Quantitative Data|quantitative data]] is data where we compare two different variables, and whether there is a [[Correlation|correlation]] between the two variables.\n\nThe *explanatory variable* (or the *independent variable*) is the variable in the bivariate distribution that we are assuming is the variable that *affects* the other variable. The *response variable* (or the *dependent variable*) is the variable in the bivariate distribution that we are assuming is the variable that is *affected by* the other variable. ^671cf7\n\n\u003e Note: Most of the content in this unit is analyzed using a *scatterplot*, which shows the data by graphing it between two axis that [[Correlation|correlate]] to the two [[Quantitative Data|quantitative]] variables.\n\n## [[Correlation]]\n\n![[Correlation#^adcde3]]\n\n## [[Linear Models]]\n![[Linear Models#^efc473]]\n\n## Residuals\nA *residual* is a value that simply describes the difference between the value of the response variable and the predicted response variable from the [[Linear Models|line of best fit]] given the explanatory variable. The notation is normally $y-\\hat{y}$.\n\n## Coefficients of Determination\nA *coefficient of determination* is the proportion of the variability in the dependent variable that is predictable from the independent variable. Usually notated as $R^2$.\n\u003e Start the \"interpretation\" sentence out with the %.\n\n- outliers and influential observations\n- transformations to achieve linearity","lastmodified":"2022-05-13T04:31:04.776679807Z","tags":null},"/Unit-8-Inference-for-Categorical-Data-for-Chi-Square":{"title":"Unit 8 - Inference for Categorical Data for Chi-Square","content":"## $\\chi^2$ Models\n$\\chi^2$ models are used to compare counts of categorical data when comparing the observed distribution(s) to the expected distribution(s) of data.\n\nThere are three types of $\\chi^2$ models:\n- One Population/Distribution Analysis\n\t- $\\chi^2$ Test for Goodness-Of-Fit ($\\chi^2$ GOF)\n\t\t- This test compares the observed counts of a *single* categorical variable to the expected distribution.\n- Comparing Two Populations/Distributions\n\t- $\\chi^2$ Test of Homogeneity\n\t\t- Compares the distribution of one categorical variable over two different populations.\n\t- $\\chi^2$ Test of Independence\n\t\t- Compares the distributions of two categorical variables over a single population.\n### Performing The Test\nThere are four general parts to creating and justifying a $\\chi^2$ test:\n- Assumptions and Conditions\n- Hypotheses\n\t- Null Hypothesis ($H_0$)\n\t- Alternate Hypothesis ($H_A$)\n- Calculations\n- Conclusions (based on $p$-value from calculation)\n\nThese factors will change slightly based on the type of $\\chi^2$ test you are performing.\n### General Notes About $\\chi^2$ Tests\nOther important facts about the $\\chi^2$ *statistic*:\n- The $\\chi^2$ statistic is used only for testing hypotheses, not for constructing confidence intervals.\n- If the observed counts don’t match the expected, the statistic will be large.\n- The $\\chi^2$ test statistic is always positive.\n\n![[Pasted image 20220418233116.png]]\n\n### Types of $\\chi^2$ Tests\n- #### [[Chi-Squared Test for Goodness-Of-Fit]]\n- #### [[Chi-Squared Test for Homogeneity]]\n- #### [[Chi-Squared Test for Independence]]\n\n## [[Linear Regression T Test|Linear Regression Tests]]\n![[Linear Models#^efc473]]\n","lastmodified":"2022-05-13T04:31:04.776679807Z","tags":null},"/Unusual-Features-of-a-Quantitative-Distribution":{"title":"Unusual Features of a Quantitative Distribution","content":"Distributions can have many unusual features, which can affect descriptions and generalizations of the distributions.\n\n## Outliers\nAn *outlier* is a value in the distribution that is extremely far away from the rest of the points in the distribution.\n\nTo identify potential outliers in the distribution, use the [[Five-Number Summary#^0d6775|IQR]] to create an \"outlier fence\". Any points outside of this \"outlier fence\" and potential outliers.\n- higher outlier fence $=Q_3 + 1.5(IQR)$\n- lower outlier fence $=Q_1 - 1.5(IQR)$\n","lastmodified":"2022-05-13T04:31:04.776679807Z","tags":null},"/Z-Scores":{"title":"Z-Scores","content":"The *$z$-score* is simply a measure of how many standard deviations away a certain value in a distribution is from the mean. ^1db326\n\n$$z_{x_i}=\\frac{x_i-\\bar{x}}{s}$$\n\nThe *$z$-score* is positive when $x$ is above the mean, and it is negative when $x$ is below the mean.\n\n$z$-scores are best used on approximately normal distributions, and aren't the best when describing a [[Shapes of Quantitative Distributions#Skew|skewed]] distribution or a distribution with [[Unusual Features of a Quantitative Distribution#Outliers|outliers]]. ^1b19e1","lastmodified":"2022-05-13T04:31:04.776679807Z","tags":null}}